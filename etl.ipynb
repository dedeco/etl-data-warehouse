{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Processes\n",
    "Use this notebook to develop the ETL process for each of your tables before completing the `etl.py` file to load the whole datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load DWH Params from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "\n",
    "HOST = config.get(\"CLUSTER\",\"HOST\")\n",
    "CLUSTER_IDENTIFIER=config.get(\"CLUSTER\",\"CLUSTER_IDENTIFIER\")\n",
    "DB_NAME = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "CLUSTER_TYPE = config.get(\"CLUSTER\",\"CLUSTER_TYPE\")\n",
    "NUM_NODES = config.get(\"CLUSTER\",\"NUM_NODES\")\n",
    "NODE_TYPE = config.get(\"CLUSTER\",\"NODE_TYPE\")\n",
    "\n",
    "IAM_ROLE_NAME = config.get(\"IAM_ROLE\", \"NAME\")\n",
    "IAM_ARN = config.get(\"IAM_ROLE\", \"ARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Connect to the clusterConnect to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Get endpoint and role_arn from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "redshift = boto3.client('redshift',\n",
    "                        region_name='us-west-2',\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET                      \n",
    "                       )\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"ENDPOINT :: \", ENDPOINT)\n",
    "print(\"ROLE_ARN :: \", ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Check out the sample data sources on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                  region_name='us-east-1',\n",
    "                  aws_access_key_id=KEY,\n",
    "                  aws_secret_access_key=SECRET\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Connect to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn_string = \"postgresql://{}:{}@{}:{}/{}\" \\\n",
    "                        .format(DB_USER, DB_PASSWORD, HOST, DB_PORT, DB_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Process `song_data`\n",
    "In this first part, you'll perform ETL on staging table. Let's perform ETL on a single song file and load a single record into each table to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songDataDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "s3_song_files = [file for _, file in map(lambda x: (x.bucket_name, x.key),songDataDbBucket.objects.filter(Prefix=\"song_dat\"))]\n",
    "print(s3_song_files[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=KEY,\n",
    "                         aws_secret_access_key=SECRET)\n",
    "obj = s3_client.get_object(Bucket='udacity-dend', Key=s3_song_files[1])\n",
    "df = pd.read_json(io.BytesIO(obj['Body'].read()),lines=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "COPY command to load data from s3://udacity-dend/song_data using your iam role credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    copy staging_artist_songs from 's3://udacity-dend/song_data'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    format as json 'auto';\n",
    "\"\"\".format(ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT * \n",
    "FROM stl_load_errors\n",
    "LIMIT 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Process `log_data`\n",
    "In this part, you'll perform ETL on the second dataset, `log_data` on on staging table. Let's perform ETL on a single log file and load a single record into each table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logDataDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "log_data_files = [file for _, file in map(lambda x: (x.bucket_name, x.key),songDataDbBucket.objects.filter(Prefix=\"log_data\"))]\n",
    "print(log_data_files[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "obj = s3_client.get_object(Bucket='udacity-dend', Key=log_data_files[1])\n",
    "json_bytes = io.BytesIO(obj['Body'].read())\n",
    "df = pd.read_json(json_bytes, lines=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#s3.create_bucket(Bucket='spartkify-etl-config', CreateBucketConfiguration={'LocationConstraint': 'us-west-2'})\n",
    "s3.Object('spartkify-etl-config', 'jpath.json').put(Body=open('jpath.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    copy staging_events \n",
    "    from 's3://udacity-dend/log_data'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS JSON 's3://spartkify-etl-config/jpath.json' ;\n",
    "\"\"\".format(ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ROLE_ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT *  FROM stl_load_errors order by starttime desc LIMIT 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT count(*)  FROM stl_load_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ELT from staging to Star schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here is a Entity Relationship Diagram for songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### ER Start Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "[<img src=\"er-sparkify.png\">]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Convert Unix timestamp in hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT  EXTRACT(hour from TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second')\n",
    "FROM staging_events\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Populate the time table with data from the staging_events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
    "SELECT TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second' AS start_time_key,\n",
    "       EXTRACT(hour from TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second')  AS hour,\n",
    "       EXTRACT(day  from TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second')  AS day,\n",
    "       EXTRACT(week from TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second')  AS week,\n",
    "       EXTRACT(month from TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second') AS month,\n",
    "       EXTRACT(year from TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second')  AS year,\n",
    "       EXTRACT(dow from TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second')   AS weekday\n",
    "FROM staging_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Populate the songs table with data from the staging_artist_songs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "INSERT INTO songs(song_id, title, artist_id, year, duration) \n",
    "SELECT DISTINCT song_id, title, artist_id, year, duration\n",
    "FROM staging_artist_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Populate the artists table with data from the staging_artist_songs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "INSERT INTO artists(artist_id, name, latitude, location, longitude)\n",
    "SELECT DISTINCT artist_id, artist_name, artist_latitude, artist_location, artist_longitude\n",
    "FROM staging_artist_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Populate the users table with data from the staging_events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT DISTINCT userid,\n",
    "    firstName,\n",
    "    lastName,\n",
    "    gender,\n",
    "    level\n",
    "FROM staging_events\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Populate the songplays table with data from the staging_events, sogns and artists table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO songplays(start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "SELECT TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second',\n",
    "    userId,\n",
    "    level,\n",
    "    s.song_id,\n",
    "    a.artist_id,\n",
    "    sessionId,\n",
    "    e.location,\n",
    "    userAgent\n",
    "FROM staging_events e\n",
    "JOIN songs s ON (e.song = s.title AND e.) \n",
    "JOIN artists a ON (e.artist = a.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Verify table staging_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT *\n",
    "FROM staging_events\n",
    "WHERE page = 'NextSong'\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
